"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.S2Stream = void 0;
const error_js_1 = require("./error.js");
const index_js_1 = require("./generated/index.js");
const factory_js_1 = require("./lib/stream/factory.js");
const shared_js_1 = require("./lib/stream/transport/fetch/shared.js");
class S2Stream {
    client;
    transportConfig;
    _transport;
    name;
    constructor(name, client, transportConfig) {
        this.name = name;
        this.client = client;
        this.transportConfig = transportConfig;
    }
    /**
     * Get or create the transport instance
     */
    async getTransport() {
        if (!this._transport) {
            this._transport = await (0, factory_js_1.createSessionTransport)(this.transportConfig);
        }
        return this._transport;
    }
    /**
     * Check the tail of the stream.
     *
     * Returns the next sequence number and timestamp to be assigned (`tail`).
     */
    async checkTail(options) {
        const response = await (0, index_js_1.checkTail)({
            client: this.client,
            path: {
                stream: this.name,
            },
            ...options,
        });
        if (response.error) {
            throw new error_js_1.S2Error({
                message: response.error.message,
                code: response.error.code ?? undefined,
                status: response.response.status,
            });
        }
        return response.data;
    }
    /**
     * Read records from the stream.
     *
     * - When `as: "bytes"` is provided, bodies and headers are decoded from base64 to `Uint8Array`.
     * - Supports starting position by `seq_num`, `timestamp`, or `tail_offset` and can clamp to the tail.
     * - Non-streaming reads are bounded by `count` and `bytes` (defaults 1000 and 1 MiB).
     * - Use `readSession` for streaming reads
     */
    async read(args, options) {
        return await (0, shared_js_1.streamRead)(this.name, this.client, args, options);
    }
    /**
     * Append one or more records to the stream.
     *
     * - Automatically base64-encodes when format is "bytes".
     * - Supports conditional appends via `fencing_token` and `match_seq_num`.
     * - Returns the acknowledged range and the stream tail after the append.
     *
     * All records in a single append call must use the same format (either all string or all bytes).
     * For high-throughput sequential appends, use `appendSession()` instead.
     *
     * @param records The record(s) to append
     * @param args Optional append arguments (fencing_token, match_seq_num)
     * @param options Optional request options
     */
    async append(records, args, options) {
        return await (0, shared_js_1.streamAppend)(this.name, this.client, records, args, options);
    }
    /**
     * Open a streaming read session
     *
     * Use the returned session as an async iterable or as a readable stream.
     * When `as: "bytes"` is provided, bodies and headers are decoded to `Uint8Array`.
     */
    async readSession(args, options) {
        const transport = await this.getTransport();
        return await transport.makeReadSession(this.name, args, options);
    }
    /**
     * Create an append session that guarantees ordering of submissions.
     *
     * Use this to coordinate high-throughput, sequential appends with backpressure.
     * Records can be either string or bytes format - the format is specified in each record.
     *
     * @param options Optional request options
     */
    async appendSession(sessionOptions, requestOptions) {
        const transport = await this.getTransport();
        return await transport.makeAppendSession(this.name, sessionOptions, requestOptions);
    }
}
exports.S2Stream = S2Stream;
//# sourceMappingURL=stream.js.map